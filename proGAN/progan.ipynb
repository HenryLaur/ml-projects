{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom math import log2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-15T20:10:33.856131Z","iopub.execute_input":"2023-07-15T20:10:33.856524Z","iopub.status.idle":"2023-07-15T20:10:37.241483Z","shell.execute_reply.started":"2023-07-15T20:10:33.856492Z","shell.execute_reply":"2023-07-15T20:10:37.240504Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n\nclass WSConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel=3, stride=1, padding=1, gain=2):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel, stride, padding)\n        self.scale = (gain / (in_channels * kernel ** 2)) ** 0.5\n        self.bias = self.conv.bias\n        self.conv.bias = None\n        \n        # initial conv layer\n        nn.init.normal_(self.conv.weight)\n        nn.init.zeros_(self.bias)\n        \n    def forward(self, x):\n        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n        \n\nclass PixelNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.epsilon = 1e-8\n        \n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n        super().__init__()\n        self.conv1 = WSConv2d(in_channels, out_channels)\n        self.conv2 = WSConv2d(out_channels, out_channels)\n        self.leaky = nn.LeakyReLU(0.2)\n        self.pn = PixelNorm()\n        self.use_pixelnorm = use_pixelnorm\n        \n    def forward(self, x):\n        x = self.leaky(self.conv1(x))\n        x = self.pn (x) if self.use_pixelnorm else x\n        x = self.leaky(self.conv2(x))\n        x = self.pn (x) if self.use_pixelnorm else x\n        return x\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim, in_channels, img_channels=3):\n        super().__init__()\n        self.initial = nn.Sequential(\n            PixelNorm(),\n            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0), # 1x1 -> 4x4\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel=3, stride=1, padding=1),\n            PixelNorm(),\n        )\n    \n        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel=1, stride=1, padding=0)\n        \n        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n        \n        for i in range(len(factors) - 1):\n            conv_in_c = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i + 1])\n            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel=1, stride=1, padding=0))\n    \n    def fade_in(self, alpha, upscaled, generated):\n        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n        \n    def forward(self, x, alpha, steps): # steps=0 (4x4), steps=1 (8x8)\n        out = self.initial(x) # 4x4\n        \n        if steps == 0:\n            return self.initial_rgb(out)\n        \n        for step in range(steps):\n            upscaled = F.interpolate(out, scale_factor=2, mode='nearest')\n            out = self.prog_blocks[step](upscaled)\n            \n        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n        final_out = self.rgb_layers[steps](out)\n        return self.fade_in(alpha, final_upscaled, final_out)\n\nclass Discriminator(nn.Module):\n    def __init__(self, z_dim, in_channels, img_channels=3):\n        super().__init__()\n        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n        \n        self.leaky = nn.LeakyReLU(0.2)\n        \n        for i in range(len(factors) - 1, 0, -1):\n            conv_in_c = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i-1])\n            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pixelnorm=False))\n            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel=1, stride=1, padding=0))\n        # For 4x4     \n        self.inital_rgb = WSConv2d(img_channels, in_channels, kernel=1, stride=1, padding=0)\n        self.rgb_layers.append(self.inital_rgb)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n        self.final_block = nn.Sequential(\n            WSConv2d(in_channels +1, in_channels, kernel=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel=4, stride=1, padding=0),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, 1, kernel=1, stride=1, padding=0)\n        )\n    \n    def fade_in(self, alpha, downscaled, out):\n        return alpha * out + (1 - alpha) * downscaled\n    \n    def minibatch_std(self, x):\n        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n        return torch.cat([x, batch_statistics], dim=1) # 512 -> 513\n        \n    \n    def forward(self, x, alpha, steps): # steps=0 (4x4), steps=1 (8x8)\n        cur_step = len(self.prog_blocks) - steps\n        out = self.leaky(self.rgb_layers[cur_step](x))\n        \n        if steps == 0:\n            out = self.minibatch_std(out)\n            return self.final_block(out).view(out.shape[0], -1)\n        \n        downscaled = self.leaky(self.rgb_layers[cur_step +1](self.avg_pool(x)))\n        out = self.avg_pool(self.prog_blocks[cur_step](out))\n        out = self.fade_in(alpha, downscaled, out)\n        \n        for step in range(cur_step + 1, len(self.prog_blocks)):\n            out = self.prog_blocks[step](out)\n            out = self.avg_pool(out)\n            \n        out = self.minibatch_std(out)\n        return self.final_block(out).view(out.shape[0], -1)\n                                          \nZ_DIM = 50\nIN_CHANNELS = 256\ngen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\ncritic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n\nfor img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n    num_steps = int(log2(img_size / 4))\n    x = torch.randn((1, Z_DIM, 1, 1))\n    z = gen(x, 0.5, steps=num_steps)\n    assert z.shape == (1, 3, img_size, img_size)\n    out = critic(z, alpha=0.5, steps=num_steps)\n    assert out.shape == (1, 1)\n    print(f\"Success! At img size: {img_size}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:10:37.243605Z","iopub.execute_input":"2023-07-15T20:10:37.244224Z","iopub.status.idle":"2023-07-15T20:10:42.633076Z","shell.execute_reply.started":"2023-07-15T20:10:37.244189Z","shell.execute_reply":"2023-07-15T20:10:42.631918Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Success! At img size: 4\nSuccess! At img size: 8\nSuccess! At img size: 16\nSuccess! At img size: 32\nSuccess! At img size: 64\nSuccess! At img size: 128\nSuccess! At img size: 256\nSuccess! At img size: 512\nSuccess! At img size: 1024\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\n\nSTART_TRAIN_AT_IMG_SIZE = 8\nDATASET = '/kaggle/input/celebahq/celeba_hq'\nCHECKPOINT_GEN = \"generator.pth\"\nCHECKPOINT_CRITIC = \"critic.pth\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSAVE_MODEL = True\nLOAD_MODEL = True\nLEARNING_RATE = 1e-3\nBATCH_SIZES = [32, 32, 32, 16, 16, 16, 16, 8, 4]\nCHANNELS_IMG = 3\nZ_DIM = 256  # should be 512 in original paper\nIN_CHANNELS = 256  # should be 512 in original paper\nCRITIC_ITERATIONS = 1\nLAMBDA_GP = 10\nPROGRESSIVE_EPOCHS = [5] * len(BATCH_SIZES)\nFIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\nNUM_WORKERS = 2","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:57:01.943319Z","iopub.execute_input":"2023-07-15T20:57:01.943709Z","iopub.status.idle":"2023-07-15T20:57:01.953791Z","shell.execute_reply.started":"2023-07-15T20:57:01.943650Z","shell.execute_reply":"2023-07-15T20:57:01.952706Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport os\nimport torchvision\nfrom torchvision.utils import save_image\nfrom scipy.stats import truncnorm\n\n!pip install wandb\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nwandb.login(key=UserSecretsClient().get_secret(\"wandb_api\"))\nrun = wandb.init(\n    # Set the project where this run will be logged\n    project=\"my-awesome-project\",\n    # Track hyperparameters and run metadata\n    config={\n        \"learning_rate\": LEARNING_RATE,\n        \"PROGRESSIVE_EPOCHS\": PROGRESSIVE_EPOCHS,\n        \"batch_size\": BATCH_SIZES\n    })\n\n\n\n# Print losses occasionally and print to tensorboard\ndef plot_to_tensorboard(\n    loss_critic, loss_gen, real, fake, tensorboard_step\n):\n    #writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n    wandb.log({'Loss Critic': loss_critic, \"tensorboard_step\": tensorboard_step })\n\n    with torch.no_grad():\n        # take out (up to) 8 examples to plot\n        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n        #writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n        #writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n        img_grid_real = wandb.Image(\n            img_grid_real\n        )\n        img_grid_fake = wandb.Image(\n            img_grid_fake\n        )\n        wandb.log({\"Real\": img_grid_real})\n        wandb.log({\"Fake\": img_grid_fake})\n\ndef gradient_penalty(critic, real, fake, alpha, train_step, device=\"cuda\"):\n    BATCH_SIZE, C, H, W = real.shape\n    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n    interpolated_images = real * beta + fake.detach() * (1 - beta)\n    interpolated_images.requires_grad_(True)\n\n    # Calculate critic scores\n    mixed_scores = critic(interpolated_images, alpha, train_step)\n\n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        inputs=interpolated_images,\n        outputs=mixed_scores,\n        grad_outputs=torch.ones_like(mixed_scores),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    gradient = gradient.view(gradient.shape[0], -1)\n    gradient_norm = gradient.norm(2, dim=1)\n    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n    return gradient_penalty\n\n\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef generate_examples(gen, steps, truncation=0.7, n=100):\n    \"\"\"\n    Tried using truncation trick here but not sure it actually helped anything, you can\n    remove it if you like and just sample from torch.randn\n    \"\"\"\n    gen.eval()\n    alpha = 1.0\n    for i in range(n):\n        with torch.no_grad():\n            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, config.Z_DIM, 1, 1)), device=config.DEVICE, dtype=torch.float32)\n            img = gen(noise, alpha, steps)\n            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n    gen.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:10:45.456842Z","iopub.execute_input":"2023-07-15T20:10:45.457415Z","iopub.status.idle":"2023-07-15T20:11:33.104404Z","shell.execute_reply.started":"2023-07-15T20:10:45.457370Z","shell.execute_reply":"2023-07-15T20:11:33.103474Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.5)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhenry-laur\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230715_201102-fu1yge93</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/henry-laur/my-awesome-project/runs/fu1yge93' target=\"_blank\">noble-spaceship-8</a></strong> to <a href='https://wandb.ai/henry-laur/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/henry-laur/my-awesome-project' target=\"_blank\">https://wandb.ai/henry-laur/my-awesome-project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/henry-laur/my-awesome-project/runs/fu1yge93' target=\"_blank\">https://wandb.ai/henry-laur/my-awesome-project/runs/fu1yge93</a>"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nprint(DEVICE)\n#torch.backends.cudnn.benchmarks = True\n\ndef get_loader(image_size):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Normalize(\n                [0.5 for _ in range(CHANNELS_IMG)],\n                [0.5 for _ in range(CHANNELS_IMG)],\n            ),\n        ]\n    )\n    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n    return loader, dataset\n\ndef train_fn(\n        critic,\n        gen,\n        loader,\n        dataset,\n        step,\n        alpha,\n        optim_critic,\n        optim_gen,\n        tensorboard_step,\n        scaler_gen,\n        scaler_critic\n    ):\n    loop = tqdm(loader, leave=True)\n    \n    for idx, (real, _) in enumerate(loop):\n        real = real.to(DEVICE)\n        cur_batch_size = real.shape[0]\n        \n        # Train critic: max (E[critic(real) - E[critic(fake)]])\n        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n        \n        with torch.cuda.amp.autocast():\n            fake = gen(noise, alpha, step)\n            critic_real = critic(real, alpha, step)\n            critic_fake = critic(fake.detach(), alpha, step)\n            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n            loss_critic = (\n                -(torch.mean(critic_real) - torch.mean(critic_fake))\n                + LAMBDA_GP * gp\n                + (0.001 * torch.mean(critic_real ** 2))\n            )\n            \n        optim_critic.zero_grad()\n        scaler_critic.scale(loss_critic).backward()\n        scaler_critic.step(optim_critic)\n        scaler_critic.update()\n        \n        \n        # Train Generator max E[critic(gen_fake)]\n        with torch.cuda.amp.autocast():\n            gen_fake = critic(fake, alpha, step)\n            loss_gen = -torch.mean(gen_fake)\n            \n        optim_gen.zero_grad()\n        scaler_gen.scale(loss_gen).backward()\n        scaler_gen.step(optim_gen)\n        scaler_gen.update()\n        \n        \n        alpha += cur_batch_size / len(dataset) * PROGRESSIVE_EPOCHS[step] * 0.5\n        alpha = min(alpha, 1)\n        \n        if(idx == len(loop) - 1):\n            with torch.no_grad():\n                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n            plot_to_tensorboard(loss_critic.item(), loss_gen.item(), real.detach(), fixed_fakes.detach(), tensorboard_step)\n            tensorboard_step += 1\n\n    return tensorboard_step, alpha\n    \ndef main():\n    gen = Generator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n    critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n    \n    optim_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n    optim_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n    \n    scaler_critic = torch.cuda.amp.GradScaler()\n    scaler_gen = torch.cuda.amp.GradScaler()\n    \n    if LOAD_MODEL:\n        load_checkpoint(\n            CHECKPOINT_GEN, gen, optim_gen, LEARNING_RATE\n        )\n        load_checkpoint(\n            CHECKPOINT_CRITIC, critic, optim_critic, LEARNING_RATE\n        )\n    gen.train()\n    critic.train()\n    \n    tensorboard_step = 0\n    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n        alpha = 1e-5\n        print(f\"Image Size: {4*2**step}, for: {num_epochs}\")\n        loader, dataset = get_loader(4 * 2 ** step)\n        \n        for epoch in range(num_epochs):\n            tensorboard_step, alpha = train_fn(\n                critic,\n                gen,\n                loader,\n                dataset,\n                step,\n                alpha,\n                optim_critic,\n                optim_gen,\n                tensorboard_step,\n                scaler_gen,\n                scaler_critic\n            )\n            \n            if SAVE_MODEL:\n                save_checkpoint(gen, optim_gen, filename=CHECKPOINT_GEN)\n                save_checkpoint(critic, optim_critic, filename=CHECKPOINT_CRITIC)\n                wandb.save('/kaggle/working/*pth*')\n        step += 1  # progress to the next img size\nmain()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:57:05.415425Z","iopub.execute_input":"2023-07-15T20:57:05.415825Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda\n=> Loading checkpoint\n=> Loading checkpoint\nImage Size: 8, for: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:30<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:33<00:00,  3.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:28<00:00,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:34<00:00,  3.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:32<00:00,  3.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\nImage Size: 16, for: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:47<00:00,  3.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:48<00:00,  3.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:49<00:00,  3.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:48<00:00,  3.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [04:46<00:00,  3.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\nImage Size: 32, for: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [06:08<00:00,  5.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [06:10<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [06:05<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [06:10<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [05:59<00:00,  5.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\nImage Size: 64, for: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [10:30<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [10:31<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [10:31<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 844/1875 [04:44<05:45,  2.98it/s]","output_type":"stream"}]}]}