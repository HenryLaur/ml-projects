{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-19T19:20:23.878054Z","iopub.execute_input":"2023-07-19T19:20:23.878537Z","iopub.status.idle":"2023-07-19T19:20:28.470690Z","shell.execute_reply.started":"2023-07-19T19:20:23.878492Z","shell.execute_reply":"2023-07-19T19:20:28.469676Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import save_image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTRAIN_DIR = \"/kaggle/input/carvana-image-masking-png/train_images/\"\nTRAIN_DIR_MASK = \"/kaggle/input/carvana-image-masking-png/train_masks/\"\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-4\nNUM_WORKERS = 2\nNUM_EPOCHS = 200\nLOAD_MODEL = False\nSAVE_MODEL = True\nCHECKPOINT_UNET = \"unetattention.pth.tar\"\n\nboth_transform = A.Compose(\n    [A.Resize(width=256, height=256), A.HorizontalFlip(p=0.5)], additional_targets={\"image0\": \"image\"},\n)\n\ntransform_only_input = A.Compose(\n    [\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n        ToTensorV2(),\n    ]\n)\n\ntransform_only_mask = A.Compose(\n    [\n        ToTensorV2(),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:20:28.472923Z","iopub.execute_input":"2023-07-19T19:20:28.473541Z","iopub.status.idle":"2023-07-19T19:20:30.509949Z","shell.execute_reply.started":"2023-07-19T19:20:28.473502Z","shell.execute_reply":"2023-07-19T19:20:30.508865Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport os\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass CityDataset(Dataset):\n    def __init__(self, root_dir, mask_dir):\n        self.root_dir = root_dir\n        self.mask_dir = mask_dir\n        self.list_file = os.listdir(self.root_dir)\n\n        \n    def __len__(self):\n        return len(self.list_file)\n    \n\n    def __getitem__(self, index):\n        img_file = self.list_file[index]\n        img_path = os.path.join(self.root_dir, img_file)\n        mask_path = os.path.join(self.mask_dir, img_file)\n        input_img = np.array(Image.open(img_path))\n        target_img = np.array(Image.open(mask_path[:-3] + \"png\"))\n        \n        augmentations = both_transform(image=input_img, image0=target_img)\n        input_img, target_img = augmentations['image'], augmentations['image0']\n        \n        input_img = transform_only_input(image=input_img)['image']\n        target_img = transform_only_mask(image=target_img)['image']\n        \n        return input_img, target_img","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:20:30.511551Z","iopub.execute_input":"2023-07-19T19:20:30.511920Z","iopub.status.idle":"2023-07-19T19:20:30.523568Z","shell.execute_reply.started":"2023-07-19T19:20:30.511885Z","shell.execute_reply":"2023-07-19T19:20:30.522472Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nte_dataset = CityDataset(root_dir=TRAIN_DIR, mask_dir=TRAIN_DIR_MASK)\nte_loader = DataLoader(te_dataset, batch_size=1, shuffle=True, num_workers=1)\n\nbatch= iter(te_loader)\nimages, labels = next(batch)\nimport torchvision.transforms as T\nlabels[0].shape\nprint(torch.unique(labels[0].float() * 255, return_counts=True))\ntransform = T.ToPILImage()\ntransform(labels[0].float())","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:20:30.527283Z","iopub.execute_input":"2023-07-19T19:20:30.527687Z","iopub.status.idle":"2023-07-19T19:20:31.062253Z","shell.execute_reply.started":"2023-07-19T19:20:30.527634Z","shell.execute_reply":"2023-07-19T19:20:31.060931Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(tensor([  0., 255.]), tensor([46798, 18738]))\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=L size=256x256>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAADeUlEQVR4nO2d23ajMBAExZ79/1/2PuRiY4QNRjPds6p6iZNDwky5JS7xQa0BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwHzcbrdDm0XXcYHlyi//Ntb9K5u2L+0rivNFDXk7fVycqmR4lA08HC0hchhLNRzZec4cJtLwdreJM7hEwZud5h7AFAb+CPa5y9dphZN0zRlMahCsEvDNsdPLQTgKSFXwMm7Sc/ikgfAqAdprmKS9ew6B1lqWgRcCnC9ix2GcgJx3YF+AQQAySnBOQIqBXQEGAWgZVXgnIIE9AR4BSKjDPgG37stx2AuIZkeAywho4aWQgO5PjQIQXUyFBIQaqCAglK4AqxHQYuuplICQe0Q1BARGoCfAbQSEUiMB7RZ2o7g3rmwTEDEJdBJg238IRYbAFxFvTSkBEWwFzDUCSEApAUlHgbnYCHCeAjgKBFBLQEAEngU4j4AWUV6tBAQYqCZguIFyAkYbeBJgPgUEUC8BgykoYGxK1wLmGwEVEzAWBKgLUIMAdQFqEPD4zYRHwZIJGPo+VRQwFASoC1CDAHUBah4FzHgUJAEIUBegBgHqAtQgQF2AGgSoC1CDAHUBHzD0s1IVBQwFAeoC1CDg/nLK2wEk4C5gzgBUTMDYj0z/Cpg0ABUTMBYEqAtQgwB1AUdYdr+5zt+xfy6C2AcL+icg+MGK7gKWpbWVhNE+vIdAwmM1rROwbF8NV2IsYHlsdll9GbmXnxd21wJJT5V1nQPSni7tKSDx4dqOAmZ/tnjuQ/btEpC9xoCZgPwlFqwEKFaYuO9TfiKgWWXFZxIUrTLjMgRkaw15CBAuteQgQLrS1H0OkJWhXXBMngD1emtiAer2V4dBQTH6/lfnAenlGPS/PhFaHCpK5ulMcD4D245NVrnKYnstYFFWHp2Loblmgu7V4EwGdntNmAosPO/eD5hlILy4ITKHgndNBo4ED78HqohyUEZAC3JQSUCLcFBMQGuDJXj0f76M4xKW11ubCDh9R+hNW/eNivBxqXsSlkNb+Ti6WMeqwVPLdbgIGHRT9HQ7Lv2PEWDTzQdc/Ofo8+c4+xs4E19gdxbw8aL597hP/wkCjJrtkZCArQEnJ4oh4NR/UjFvz5d05CSg87lvF9LK+QmBWf8AAAAAAAAAAAAAAAAAAAAAAP87/wCgt1kuGoJZ2gAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torchvision.utils import save_image\n\ndef save_some_examples(unet, val_loader, epoch, folder):\n    x, y = next(iter(val_loader))\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    unet.eval()\n    with torch.no_grad():\n        y_fake = unet(x)\n        y_fake[y_fake > 0.5] = 1  # remove normalization#\n        y_fake[y_fake <= 0.5] = 0  # remove normalization#\n        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n        save_image(y.float() , folder + f\"/label_{epoch}.png\")\n    unet.train()\n\n\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(\"/kaggle/working/\" + checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:20:31.068205Z","iopub.execute_input":"2023-07-19T19:20:31.071186Z","iopub.status.idle":"2023-07-19T19:20:31.090060Z","shell.execute_reply.started":"2023-07-19T19:20:31.071134Z","shell.execute_reply":"2023-07-19T19:20:31.089018Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        return self.conv(x)\n    \nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = ConvBlock(in_channels, out_channels)\n        self.pool = nn.MaxPool2d(2, 2)\n    \n    def forward(self, x):\n        s = self.conv(x)\n        p = self.pool(s)\n        return s, p\n\n    \nclass AttentionGate(nn.Module):\n    def __init__(self, in_channels_input, in_channels_skip, out_channels):\n        super().__init__()\n        self.input_conv = nn.Sequential(\n            nn.Conv2d(in_channels_input, out_channels, kernel_size=1, padding=0, stride=1),\n            nn.BatchNorm2d(out_channels)\n        )\n        \n        self.skip_conv = nn.Sequential(\n            nn.Conv2d(in_channels_skip, out_channels, kernel_size=1, padding=0, stride=1),\n            nn.BatchNorm2d(out_channels)\n        )\n        \n        self.relu = nn.ReLU(inplace=True)\n        \n        self.output_conv = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, stride=1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, inputs, skips):\n        x = self.input_conv(inputs)\n        s = self.skip_conv(skips)\n        out = self.relu(x + s)\n        out = self.output_conv(out)\n        return out * skips\n        \nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels_input, in_channels_skip, out_channels):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.attention = AttentionGate(in_channels_input, in_channels_skip, out_channels)\n        self.c1 = ConvBlock(in_channels_input + out_channels, out_channels)\n        \n        \n    def forward(self, x, skip):\n\n        x = self.up(x)\n        skip = self.attention(x, skip)\n        x = torch.cat([x, skip], dim=1)\n        return self.c1(x)\n        \n    \nclass AttentionUnet(nn.Module):\n    def __init__(self, in_channels, out_channels, feature=64):\n        super().__init__()\n        self.e1 = EncoderBlock(in_channels, feature)\n        self.e2 = EncoderBlock(feature, feature * 2)\n        self.e3 = EncoderBlock(feature * 2, feature * 4)\n        \n        self.b1 = ConvBlock(feature * 4, feature * 8)\n        \n        self.d1 = DecoderBlock(feature * 8, feature * 4, feature * 4)\n        self.d2 = DecoderBlock(feature * 4, feature * 2, feature * 2)\n        self.d3 = DecoderBlock(feature * 2, feature, feature)\n    \n        self.final = nn.Conv2d(feature, out_channels, kernel_size=1, padding=0)\n    \n    def forward(self, x):\n        s, p = self.e1(x)\n        s2, p2 = self.e2(p)\n        s3, p3 = self.e3(p2)\n        \n        b1 = self.b1(p3)\n        \n        d1 = self.d1(b1, s3)\n        d2 = self.d2(d1, s2)\n        d3 = self.d3(d2, s)\n        \n        return self.final(d3)\n        \nx = torch.randn((5, 3, 64, 64))\nmodel = AttentionUnet(3, 3)\nprint(model(x).shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:23:56.472629Z","iopub.execute_input":"2023-07-19T19:23:56.473034Z","iopub.status.idle":"2023-07-19T19:23:56.959359Z","shell.execute_reply.started":"2023-07-19T19:23:56.473003Z","shell.execute_reply":"2023-07-19T19:23:56.958183Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"torch.Size([5, 3, 64, 64])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image\nprint(SAVE_MODEL)\ntorch.backends.cudnn.benchmark = True\n\ndef train_fn(unet, train_loader, opt, loss_fn, scaler):\n    loop = tqdm(train_loader, leave=True)\n    \n    for idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.float().to(DEVICE)\n        \n        #Train Discriminator\n        with torch.cuda.amp.autocast():\n            segment = unet(x)\n            loss = loss_fn(segment, y)\n        opt.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        loop.set_postfix(loss=loss.item())\n\n        \n        \ndef main():\n    unet = AttentionUnet(in_channels=3, out_channels=1).to(DEVICE)\n    opt=optim.Adam(unet.parameters(), lr=LEARNING_RATE)\n    \n    loss = nn.BCEWithLogitsLoss()\n    \n    if LOAD_MODEL:\n        load_checkpoint(CHECKPOINT_UNET, unet, opt, LEARNING_RATE)\n        \n    train_dataset = CityDataset(root_dir=TRAIN_DIR, mask_dir=TRAIN_DIR_MASK)\n    train_subset, val_subset = torch.utils.data.random_split(\n        train_dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(1))\n    scaler = torch.cuda.amp.GradScaler()\n    \n    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n    \n    for epoch in range(NUM_EPOCHS):\n        train_fn(unet, train_loader, opt, loss, scaler)\n        \n        if(SAVE_MODEL and epoch % 5 == 0):\n            save_checkpoint(unet, opt, filename=CHECKPOINT_UNET)\n        if epoch % 5 == 0:\n            save_some_examples(unet, val_loader, epoch, folder='.')\n\nmain()","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:24:02.682536Z","iopub.execute_input":"2023-07-19T19:24:02.683040Z","iopub.status.idle":"2023-07-19T20:04:10.461331Z","shell.execute_reply.started":"2023-07-19T19:24:02.683000Z","shell.execute_reply":"2023-07-19T20:04:10.458428Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 255/255 [02:56<00:00,  1.44it/s, loss=0.119]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0778]\n100%|██████████| 255/255 [02:44<00:00,  1.55it/s, loss=0.0516]\n100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0361]\n100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0284]\n100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0204]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0172]\n100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.0129]\n100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.0125]\n100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0117]\n100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.00916]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0214] \n100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.009]  \n100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.0079] \n 58%|█████▊    | 148/255 [01:35<01:09,  1.55it/s, loss=0.00728]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m             save_some_examples(unet, val_loader, epoch, folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 46\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_subset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mNUM_WORKERS)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(SAVE_MODEL \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     49\u001b[0m         save_checkpoint(unet, opt, filename\u001b[38;5;241m=\u001b[39mCHECKPOINT_UNET)\n","Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(unet, train_loader, opt, loss_fn, scaler)\u001b[0m\n\u001b[1;32m     20\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     24\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:370\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 370\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    290\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    290\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"\nunet = AttentionUnet(in_channels=3, out_channels=1).to(DEVICE)\nfrom torchsummary import summary\n\n\nsummary(unet, (3, 256, 256))","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:23:35.426850Z","iopub.execute_input":"2023-07-19T19:23:35.427221Z","iopub.status.idle":"2023-07-19T19:23:36.554108Z","shell.execute_reply.started":"2023-07-19T19:23:35.427188Z","shell.execute_reply":"2023-07-19T19:23:36.552888Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 256, 256]           1,792\n       BatchNorm2d-2         [-1, 64, 256, 256]             128\n              ReLU-3         [-1, 64, 256, 256]               0\n            Conv2d-4         [-1, 64, 256, 256]          36,928\n       BatchNorm2d-5         [-1, 64, 256, 256]             128\n              ReLU-6         [-1, 64, 256, 256]               0\n         ConvBlock-7         [-1, 64, 256, 256]               0\n         MaxPool2d-8         [-1, 64, 128, 128]               0\n      EncoderBlock-9  [[-1, 64, 256, 256], [-1, 64, 128, 128]]               0\n           Conv2d-10        [-1, 128, 128, 128]          73,856\n      BatchNorm2d-11        [-1, 128, 128, 128]             256\n             ReLU-12        [-1, 128, 128, 128]               0\n           Conv2d-13        [-1, 128, 128, 128]         147,584\n      BatchNorm2d-14        [-1, 128, 128, 128]             256\n             ReLU-15        [-1, 128, 128, 128]               0\n        ConvBlock-16        [-1, 128, 128, 128]               0\n        MaxPool2d-17          [-1, 128, 64, 64]               0\n     EncoderBlock-18  [[-1, 128, 128, 128], [-1, 128, 64, 64]]               0\n           Conv2d-19          [-1, 256, 64, 64]         295,168\n      BatchNorm2d-20          [-1, 256, 64, 64]             512\n             ReLU-21          [-1, 256, 64, 64]               0\n           Conv2d-22          [-1, 256, 64, 64]         590,080\n      BatchNorm2d-23          [-1, 256, 64, 64]             512\n             ReLU-24          [-1, 256, 64, 64]               0\n        ConvBlock-25          [-1, 256, 64, 64]               0\n        MaxPool2d-26          [-1, 256, 32, 32]               0\n     EncoderBlock-27  [[-1, 256, 64, 64], [-1, 256, 32, 32]]               0\n           Conv2d-28          [-1, 512, 32, 32]       1,180,160\n      BatchNorm2d-29          [-1, 512, 32, 32]           1,024\n             ReLU-30          [-1, 512, 32, 32]               0\n           Conv2d-31          [-1, 512, 32, 32]       2,359,808\n      BatchNorm2d-32          [-1, 512, 32, 32]           1,024\n             ReLU-33          [-1, 512, 32, 32]               0\n        ConvBlock-34          [-1, 512, 32, 32]               0\n        MaxPool2d-35          [-1, 512, 16, 16]               0\n     EncoderBlock-36  [[-1, 512, 32, 32], [-1, 512, 16, 16]]               0\n           Conv2d-37         [-1, 1024, 16, 16]       4,719,616\n      BatchNorm2d-38         [-1, 1024, 16, 16]           2,048\n             ReLU-39         [-1, 1024, 16, 16]               0\n           Conv2d-40         [-1, 1024, 16, 16]       9,438,208\n      BatchNorm2d-41         [-1, 1024, 16, 16]           2,048\n             ReLU-42         [-1, 1024, 16, 16]               0\n        ConvBlock-43         [-1, 1024, 16, 16]               0\n         Upsample-44         [-1, 1024, 32, 32]               0\n           Conv2d-45          [-1, 512, 32, 32]         524,800\n      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n           Conv2d-47          [-1, 512, 32, 32]         262,656\n      BatchNorm2d-48          [-1, 512, 32, 32]           1,024\n             ReLU-49          [-1, 512, 32, 32]               0\n           Conv2d-50          [-1, 512, 32, 32]         262,656\n          Sigmoid-51          [-1, 512, 32, 32]               0\n    AttentionGate-52          [-1, 512, 32, 32]               0\n           Conv2d-53          [-1, 512, 32, 32]       7,078,400\n      BatchNorm2d-54          [-1, 512, 32, 32]           1,024\n             ReLU-55          [-1, 512, 32, 32]               0\n           Conv2d-56          [-1, 512, 32, 32]       2,359,808\n      BatchNorm2d-57          [-1, 512, 32, 32]           1,024\n             ReLU-58          [-1, 512, 32, 32]               0\n        ConvBlock-59          [-1, 512, 32, 32]               0\n     DecoderBlock-60          [-1, 512, 32, 32]               0\n         Upsample-61          [-1, 512, 64, 64]               0\n           Conv2d-62          [-1, 256, 64, 64]         131,328\n      BatchNorm2d-63          [-1, 256, 64, 64]             512\n           Conv2d-64          [-1, 256, 64, 64]          65,792\n      BatchNorm2d-65          [-1, 256, 64, 64]             512\n             ReLU-66          [-1, 256, 64, 64]               0\n           Conv2d-67          [-1, 256, 64, 64]          65,792\n          Sigmoid-68          [-1, 256, 64, 64]               0\n    AttentionGate-69          [-1, 256, 64, 64]               0\n           Conv2d-70          [-1, 256, 64, 64]       1,769,728\n      BatchNorm2d-71          [-1, 256, 64, 64]             512\n             ReLU-72          [-1, 256, 64, 64]               0\n           Conv2d-73          [-1, 256, 64, 64]         590,080\n      BatchNorm2d-74          [-1, 256, 64, 64]             512\n             ReLU-75          [-1, 256, 64, 64]               0\n        ConvBlock-76          [-1, 256, 64, 64]               0\n     DecoderBlock-77          [-1, 256, 64, 64]               0\n         Upsample-78        [-1, 256, 128, 128]               0\n           Conv2d-79        [-1, 128, 128, 128]          32,896\n      BatchNorm2d-80        [-1, 128, 128, 128]             256\n           Conv2d-81        [-1, 128, 128, 128]          16,512\n      BatchNorm2d-82        [-1, 128, 128, 128]             256\n             ReLU-83        [-1, 128, 128, 128]               0\n           Conv2d-84        [-1, 128, 128, 128]          16,512\n          Sigmoid-85        [-1, 128, 128, 128]               0\n    AttentionGate-86        [-1, 128, 128, 128]               0\n           Conv2d-87        [-1, 128, 128, 128]         442,496\n      BatchNorm2d-88        [-1, 128, 128, 128]             256\n             ReLU-89        [-1, 128, 128, 128]               0\n           Conv2d-90        [-1, 128, 128, 128]         147,584\n      BatchNorm2d-91        [-1, 128, 128, 128]             256\n             ReLU-92        [-1, 128, 128, 128]               0\n        ConvBlock-93        [-1, 128, 128, 128]               0\n     DecoderBlock-94        [-1, 128, 128, 128]               0\n         Upsample-95        [-1, 128, 256, 256]               0\n           Conv2d-96         [-1, 64, 256, 256]           8,256\n      BatchNorm2d-97         [-1, 64, 256, 256]             128\n           Conv2d-98         [-1, 64, 256, 256]           4,160\n      BatchNorm2d-99         [-1, 64, 256, 256]             128\n            ReLU-100         [-1, 64, 256, 256]               0\n          Conv2d-101         [-1, 64, 256, 256]           4,160\n         Sigmoid-102         [-1, 64, 256, 256]               0\n   AttentionGate-103         [-1, 64, 256, 256]               0\n          Conv2d-104         [-1, 64, 256, 256]         110,656\n     BatchNorm2d-105         [-1, 64, 256, 256]             128\n            ReLU-106         [-1, 64, 256, 256]               0\n          Conv2d-107         [-1, 64, 256, 256]          36,928\n     BatchNorm2d-108         [-1, 64, 256, 256]             128\n            ReLU-109         [-1, 64, 256, 256]               0\n       ConvBlock-110         [-1, 64, 256, 256]               0\n    DecoderBlock-111         [-1, 64, 256, 256]               0\n          Conv2d-112          [-1, 1, 256, 256]              65\n================================================================\nTotal params: 32,790,081\nTrainable params: 32,790,081\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 44562950.50\nParams size (MB): 125.08\nEstimated Total Size (MB): 44563076.33\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        return self.conv(x)\n    \nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = ConvBlock(in_channels, out_channels)\n        self.pool = nn.MaxPool2d(2, 2)\n    \n    def forward(self, x):\n        s = self.conv(x)\n        p = self.pool(s)\n        return s, p\n\n    \nclass AttentionGate(nn.Module):\n    def __init__(self, in_channels_input, in_channels_skip, out_channels):\n        super().__init__()\n        self.input_conv = nn.Sequential(\n            nn.Conv2d(in_channels_input, out_channels, kernel_size=1, padding=0, stride=1),\n            nn.BatchNorm2d(out_channels)\n        )\n        \n        self.skip_conv = nn.Sequential(\n            nn.Conv2d(in_channels_skip, out_channels, kernel_size=1, padding=0, stride=1),\n            nn.BatchNorm2d(out_channels)\n        )\n        \n        self.relu = nn.ReLU(inplace=True)\n        \n        self.output_conv = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, stride=1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, inputs, skips):\n        x = self.input_conv(inputs)\n        s = self.skip_conv(skips)\n        out = self.relu(x + s)\n        out = self.output_conv(out)\n        return out * skips\n        \nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels_input, in_channels_skip, out_channels):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.attention = AttentionGate(in_channels_input, in_channels_skip, out_channels)\n        self.c1 = ConvBlock(in_channels_input + out_channels, out_channels)\n        \n        \n    def forward(self, x, skip):\n\n        x = self.up(x)\n        skip = self.attention(x, skip)\n        x = torch.cat([x, skip], dim=1)\n        return self.c1(x)\n        \n    \nclass AttentionUnet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, features=[64, 128, 256, 512]):\n        super().__init__()\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        \n        for feature in features:\n            self.downs.append(EncoderBlock(in_channels, feature))\n            in_channels = feature\n                              \n\n        self.b1 = ConvBlock(features[-1], features[-1] * 2)\n        \n        for feature in reversed(features):\n            self.ups.append(DecoderBlock(feature * 2, feature, feature))\n            \n        self.final = nn.Conv2d(features[0], out_channels, kernel_size=1, padding=0)\n    \n    def forward(self, x):\n        skips = []\n        for down in self.downs:\n            s, x = down(x)\n            skips.append(s)\n        \n        x = self.b1(x)\n        \n        for up in self.ups:\n            a = skips.pop()\n            x = up(x, a)\n        \n        return self.final(x)\n        \nx = torch.randn((5, 3, 64, 64))\nmodel = AttentionUnet(3, 3)\nprint(model(x).shape)","metadata":{},"execution_count":null,"outputs":[]}]}