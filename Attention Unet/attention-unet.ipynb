{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "execution": {
                    "iopub.execute_input": "2023-07-19T19:20:23.878537Z",
                    "iopub.status.busy": "2023-07-19T19:20:23.878054Z",
                    "iopub.status.idle": "2023-07-19T19:20:28.470690Z",
                    "shell.execute_reply": "2023-07-19T19:20:28.469676Z",
                    "shell.execute_reply.started": "2023-07-19T19:20:23.878492Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import nn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-07-19T19:20:28.473541Z",
                    "iopub.status.busy": "2023-07-19T19:20:28.472923Z",
                    "iopub.status.idle": "2023-07-19T19:20:30.509949Z",
                    "shell.execute_reply": "2023-07-19T19:20:30.508865Z",
                    "shell.execute_reply.started": "2023-07-19T19:20:28.473502Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
                        "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
                    ]
                }
            ],
            "source": [
                "from torchvision.utils import save_image\n",
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "TRAIN_DIR = \"/kaggle/input/carvana-image-masking-png/train_images/\"\n",
                "TRAIN_DIR_MASK = \"/kaggle/input/carvana-image-masking-png/train_masks/\"\n",
                "BATCH_SIZE = 16\n",
                "LEARNING_RATE = 1e-4\n",
                "NUM_WORKERS = 2\n",
                "NUM_EPOCHS = 200\n",
                "LOAD_MODEL = False\n",
                "SAVE_MODEL = True\n",
                "CHECKPOINT_UNET = \"unetattention.pth.tar\"\n",
                "\n",
                "both_transform = A.Compose(\n",
                "    [A.Resize(width=256, height=256), A.HorizontalFlip(p=0.5)], additional_targets={\"image0\": \"image\"},\n",
                ")\n",
                "\n",
                "transform_only_input = A.Compose(\n",
                "    [\n",
                "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
                "        ToTensorV2(),\n",
                "    ]\n",
                ")\n",
                "\n",
                "transform_only_mask = A.Compose(\n",
                "    [\n",
                "        ToTensorV2(),\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-07-19T19:20:30.511920Z",
                    "iopub.status.busy": "2023-07-19T19:20:30.511551Z",
                    "iopub.status.idle": "2023-07-19T19:20:30.523568Z",
                    "shell.execute_reply": "2023-07-19T19:20:30.522472Z",
                    "shell.execute_reply.started": "2023-07-19T19:20:30.511885Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "import os\n",
                "from torch.utils.data import Dataset\n",
                "import numpy as np\n",
                "\n",
                "class CityDataset(Dataset):\n",
                "    def __init__(self, root_dir, mask_dir):\n",
                "        self.root_dir = root_dir\n",
                "        self.mask_dir = mask_dir\n",
                "        self.list_file = os.listdir(self.root_dir)\n",
                "\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.list_file)\n",
                "    \n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        img_file = self.list_file[index]\n",
                "        img_path = os.path.join(self.root_dir, img_file)\n",
                "        mask_path = os.path.join(self.mask_dir, img_file)\n",
                "        input_img = np.array(Image.open(img_path))\n",
                "        target_img = np.array(Image.open(mask_path[:-3] + \"png\"))\n",
                "        \n",
                "        augmentations = both_transform(image=input_img, image0=target_img)\n",
                "        input_img, target_img = augmentations['image'], augmentations['image0']\n",
                "        \n",
                "        input_img = transform_only_input(image=input_img)['image']\n",
                "        target_img = transform_only_mask(image=target_img)['image']\n",
                "        \n",
                "        return input_img, target_img"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-07-19T19:20:30.527687Z",
                    "iopub.status.busy": "2023-07-19T19:20:30.527283Z",
                    "iopub.status.idle": "2023-07-19T19:20:31.062253Z",
                    "shell.execute_reply": "2023-07-19T19:20:31.060931Z",
                    "shell.execute_reply.started": "2023-07-19T19:20:30.527634Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(tensor([  0., 255.]), tensor([46798, 18738]))\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAADeUlEQVR4nO2d23ajMBAExZ79/1/2PuRiY4QNRjPds6p6iZNDwky5JS7xQa0BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwHzcbrdDm0XXcYHlyi//Ntb9K5u2L+0rivNFDXk7fVycqmR4lA08HC0hchhLNRzZec4cJtLwdreJM7hEwZud5h7AFAb+CPa5y9dphZN0zRlMahCsEvDNsdPLQTgKSFXwMm7Sc/ikgfAqAdprmKS9ew6B1lqWgRcCnC9ix2GcgJx3YF+AQQAySnBOQIqBXQEGAWgZVXgnIIE9AR4BSKjDPgG37stx2AuIZkeAywho4aWQgO5PjQIQXUyFBIQaqCAglK4AqxHQYuuplICQe0Q1BARGoCfAbQSEUiMB7RZ2o7g3rmwTEDEJdBJg238IRYbAFxFvTSkBEWwFzDUCSEApAUlHgbnYCHCeAjgKBFBLQEAEngU4j4AWUV6tBAQYqCZguIFyAkYbeBJgPgUEUC8BgykoYGxK1wLmGwEVEzAWBKgLUIMAdQFqEPD4zYRHwZIJGPo+VRQwFASoC1CDAHUBah4FzHgUJAEIUBegBgHqAtQgQF2AGgSoC1CDAHUBHzD0s1IVBQwFAeoC1CDg/nLK2wEk4C5gzgBUTMDYj0z/Cpg0ABUTMBYEqAtQgwB1AUdYdr+5zt+xfy6C2AcL+icg+MGK7gKWpbWVhNE+vIdAwmM1rROwbF8NV2IsYHlsdll9GbmXnxd21wJJT5V1nQPSni7tKSDx4dqOAmZ/tnjuQ/btEpC9xoCZgPwlFqwEKFaYuO9TfiKgWWXFZxIUrTLjMgRkaw15CBAuteQgQLrS1H0OkJWhXXBMngD1emtiAer2V4dBQTH6/lfnAenlGPS/PhFaHCpK5ulMcD4D245NVrnKYnstYFFWHp2Loblmgu7V4EwGdntNmAosPO/eD5hlILy4ITKHgndNBo4ED78HqohyUEZAC3JQSUCLcFBMQGuDJXj0f76M4xKW11ubCDh9R+hNW/eNivBxqXsSlkNb+Ti6WMeqwVPLdbgIGHRT9HQ7Lv2PEWDTzQdc/Ofo8+c4+xs4E19gdxbw8aL597hP/wkCjJrtkZCArQEnJ4oh4NR/UjFvz5d05CSg87lvF9LK+QmBWf8AAAAAAAAAAAAAAAAAAAAAAP87/wCgt1kuGoJZ2gAAAABJRU5ErkJggg==",
                        "text/plain": [
                            "<PIL.Image.Image image mode=L size=256x256>"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from torch.utils.data import DataLoader\n",
                "te_dataset = CityDataset(root_dir=TRAIN_DIR, mask_dir=TRAIN_DIR_MASK)\n",
                "te_loader = DataLoader(te_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
                "\n",
                "batch= iter(te_loader)\n",
                "images, labels = next(batch)\n",
                "import torchvision.transforms as T\n",
                "labels[0].shape\n",
                "print(torch.unique(labels[0].float() * 255, return_counts=True))\n",
                "transform = T.ToPILImage()\n",
                "transform(labels[0].float())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-07-19T19:20:31.071186Z",
                    "iopub.status.busy": "2023-07-19T19:20:31.068205Z",
                    "iopub.status.idle": "2023-07-19T19:20:31.090060Z",
                    "shell.execute_reply": "2023-07-19T19:20:31.089018Z",
                    "shell.execute_reply.started": "2023-07-19T19:20:31.071134Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "from torchvision.utils import save_image\n",
                "\n",
                "def save_some_examples(unet, val_loader, epoch, folder):\n",
                "    x, y = next(iter(val_loader))\n",
                "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
                "    unet.eval()\n",
                "    with torch.no_grad():\n",
                "        y_fake = unet(x)\n",
                "        y_fake[y_fake > 0.5] = 1  # remove normalization#\n",
                "        y_fake[y_fake <= 0.5] = 0  # remove normalization#\n",
                "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
                "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
                "        save_image(y.float() , folder + f\"/label_{epoch}.png\")\n",
                "    unet.train()\n",
                "\n",
                "\n",
                "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
                "    print(\"=> Saving checkpoint\")\n",
                "    checkpoint = {\n",
                "        \"state_dict\": model.state_dict(),\n",
                "        \"optimizer\": optimizer.state_dict(),\n",
                "    }\n",
                "    torch.save(checkpoint, filename)\n",
                "\n",
                "\n",
                "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
                "    print(\"=> Loading checkpoint\")\n",
                "    checkpoint = torch.load(\"/kaggle/working/\" + checkpoint_file, map_location=DEVICE)\n",
                "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
                "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
                "    # If we don't do this then it will just have learning rate of old checkpoint\n",
                "    # and it will lead to many hours of debugging \\:\n",
                "    for param_group in optimizer.param_groups:\n",
                "        param_group[\"lr\"] = lr\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-07-19T19:23:56.473034Z",
                    "iopub.status.busy": "2023-07-19T19:23:56.472629Z",
                    "iopub.status.idle": "2023-07-19T19:23:56.959359Z",
                    "shell.execute_reply": "2023-07-19T19:23:56.958183Z",
                    "shell.execute_reply.started": "2023-07-19T19:23:56.473003Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([5, 3, 64, 64])\n"
                    ]
                }
            ],
            "source": [
                "class ConvBlock(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.conv(x)\n",
                "    \n",
                "class EncoderBlock(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super().__init__()\n",
                "        self.conv = ConvBlock(in_channels, out_channels)\n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        s = self.conv(x)\n",
                "        p = self.pool(s)\n",
                "        return s, p\n",
                "\n",
                "    \n",
                "class AttentionGate(nn.Module):\n",
                "    def __init__(self, in_channels_input, in_channels_skip, out_channels):\n",
                "        super().__init__()\n",
                "        self.input_conv = nn.Sequential(\n",
                "            nn.Conv2d(in_channels_input, out_channels, kernel_size=1, padding=0, stride=1),\n",
                "            nn.BatchNorm2d(out_channels)\n",
                "        )\n",
                "        \n",
                "        self.skip_conv = nn.Sequential(\n",
                "            nn.Conv2d(in_channels_skip, out_channels, kernel_size=1, padding=0, stride=1),\n",
                "            nn.BatchNorm2d(out_channels)\n",
                "        )\n",
                "        \n",
                "        self.relu = nn.ReLU(inplace=True)\n",
                "        \n",
                "        self.output_conv = nn.Sequential(\n",
                "            nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, stride=1),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "        \n",
                "    def forward(self, inputs, skips):\n",
                "        x = self.input_conv(inputs)\n",
                "        s = self.skip_conv(skips)\n",
                "        out = self.relu(x + s)\n",
                "        out = self.output_conv(out)\n",
                "        return out * skips\n",
                "        \n",
                "class DecoderBlock(nn.Module):\n",
                "    def __init__(self, in_channels_input, in_channels_skip, out_channels):\n",
                "        super().__init__()\n",
                "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
                "        self.attention = AttentionGate(in_channels_input, in_channels_skip, out_channels)\n",
                "        self.c1 = ConvBlock(in_channels_input + out_channels, out_channels)\n",
                "        \n",
                "        \n",
                "    def forward(self, x, skip):\n",
                "\n",
                "        x = self.up(x)\n",
                "        skip = self.attention(x, skip)\n",
                "        x = torch.cat([x, skip], dim=1)\n",
                "        return self.c1(x)\n",
                "        \n",
                "    \n",
                "class AttentionUnet(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels, feature=64):\n",
                "        super().__init__()\n",
                "        self.e1 = EncoderBlock(in_channels, feature)\n",
                "        self.e2 = EncoderBlock(feature, feature * 2)\n",
                "        self.e3 = EncoderBlock(feature * 2, feature * 4)\n",
                "        \n",
                "        self.b1 = ConvBlock(feature * 4, feature * 8)\n",
                "        \n",
                "        self.d1 = DecoderBlock(feature * 8, feature * 4, feature * 4)\n",
                "        self.d2 = DecoderBlock(feature * 4, feature * 2, feature * 2)\n",
                "        self.d3 = DecoderBlock(feature * 2, feature, feature)\n",
                "    \n",
                "        self.final = nn.Conv2d(feature, out_channels, kernel_size=1, padding=0)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        s, p = self.e1(x)\n",
                "        s2, p2 = self.e2(p)\n",
                "        s3, p3 = self.e3(p2)\n",
                "        \n",
                "        b1 = self.b1(p3)\n",
                "        \n",
                "        d1 = self.d1(b1, s3)\n",
                "        d2 = self.d2(d1, s2)\n",
                "        d3 = self.d3(d2, s)\n",
                "        \n",
                "        return self.final(d3)\n",
                "        \n",
                "x = torch.randn((5, 3, 64, 64))\n",
                "model = AttentionUnet(3, 3)\n",
                "print(model(x).shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-07-19T19:24:02.683040Z",
                    "iopub.status.busy": "2023-07-19T19:24:02.682536Z",
                    "iopub.status.idle": "2023-07-19T20:04:10.461331Z",
                    "shell.execute_reply": "2023-07-19T20:04:10.458428Z",
                    "shell.execute_reply.started": "2023-07-19T19:24:02.683000Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "True\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 255/255 [02:56<00:00,  1.44it/s, loss=0.119]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=> Saving checkpoint\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0778]\n",
                        "100%|██████████| 255/255 [02:44<00:00,  1.55it/s, loss=0.0516]\n",
                        "100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0361]\n",
                        "100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0284]\n",
                        "100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0204]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=> Saving checkpoint\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0172]\n",
                        "100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.0129]\n",
                        "100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.0125]\n",
                        "100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0117]\n",
                        "100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.00916]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=> Saving checkpoint\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 255/255 [02:43<00:00,  1.56it/s, loss=0.0214] \n",
                        "100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.009]  \n",
                        "100%|██████████| 255/255 [02:42<00:00,  1.57it/s, loss=0.0079] \n",
                        " 58%|█████▊    | 148/255 [01:35<01:09,  1.55it/s, loss=0.00728]\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from tqdm import tqdm\n",
                "from torchvision.utils import save_image\n",
                "print(SAVE_MODEL)\n",
                "torch.backends.cudnn.benchmark = True\n",
                "\n",
                "def train_fn(unet, train_loader, opt, loss_fn, scaler):\n",
                "    loop = tqdm(train_loader, leave=True)\n",
                "    \n",
                "    for idx, (x, y) in enumerate(loop):\n",
                "        x, y = x.to(DEVICE), y.float().to(DEVICE)\n",
                "        \n",
                "        #Train Discriminator\n",
                "        with torch.cuda.amp.autocast():\n",
                "            segment = unet(x)\n",
                "            loss = loss_fn(segment, y)\n",
                "        opt.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(opt)\n",
                "        scaler.update()\n",
                "        loop.set_postfix(loss=loss.item())\n",
                "\n",
                "        \n",
                "        \n",
                "def main():\n",
                "    unet = AttentionUnet(in_channels=3, out_channels=1).to(DEVICE)\n",
                "    opt=optim.Adam(unet.parameters(), lr=LEARNING_RATE)\n",
                "    \n",
                "    loss = nn.BCEWithLogitsLoss()\n",
                "    \n",
                "    if LOAD_MODEL:\n",
                "        load_checkpoint(CHECKPOINT_UNET, unet, opt, LEARNING_RATE)\n",
                "        \n",
                "    train_dataset = CityDataset(root_dir=TRAIN_DIR, mask_dir=TRAIN_DIR_MASK)\n",
                "    train_subset, val_subset = torch.utils.data.random_split(\n",
                "        train_dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(1))\n",
                "    scaler = torch.cuda.amp.GradScaler()\n",
                "    \n",
                "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
                "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
                "    \n",
                "    for epoch in range(NUM_EPOCHS):\n",
                "        train_fn(unet, train_loader, opt, loss, scaler)\n",
                "        \n",
                "        if(SAVE_MODEL and epoch % 5 == 0):\n",
                "            save_checkpoint(unet, opt, filename=CHECKPOINT_UNET)\n",
                "        if epoch % 5 == 0:\n",
                "            save_some_examples(unet, val_loader, epoch, folder='.')\n",
                "\n",
                "main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}