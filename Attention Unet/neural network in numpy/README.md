This is a custom implementation of a neural network in numpy. It uses Binary Cross Entropy as the cost function and the architecture can be defined by a list of tuples ex layers = [(2, ""), (20, 'l_relu'), (50, 'l_relu'), (5, 'l_relu'), (1, 'sigmoid')] where the input activation is just ignored (was a bit too lazy to fix ;). Supported activations relu, tanh, sigmoid, l_relu (leaky relu).